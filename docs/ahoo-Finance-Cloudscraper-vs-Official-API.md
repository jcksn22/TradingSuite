# Yahoo Finance: Hivatalos API vs Cloudscraper √∂sszehasonl√≠t√°s

A Yahoo Finance adatainak k√©t f≈ë el√©r√©si m√≥dja van: a hivatalos, fizet≈ës API √©s a webes scraping megold√°s. Ez a dokumentum r√©szletesen √∂sszehasonl√≠tja a k√©t megk√∂zel√≠t√©st, hogy seg√≠tsen eld√∂nteni, melyik a legmegfelel≈ëbb az adott projekthez.

## üí∏ √Åraz√°si k√ºl√∂nbs√©gek

### Hivatalos Yahoo Finance API (RapidAPI-n kereszt√ºl)

| Csomag | √År/h√≥ | K√©r√©sek sz√°ma | √År/k√©r√©s |
|---------|-------------|----------|---------------|
| Basic | $0 | 500/h√≥ | Ingyenes |
| Pro | $10 | 10,000/h√≥ | $0.001 |
| Ultra | $30 | 50,000/h√≥ | $0.0006 |
| Mega | $75+ | 100,000+/h√≥ | $0.00075 |

### Cloudscraper "k√∂lts√©gek"

| Szempont | K√∂lts√©g | Megjegyz√©s |
|--------|------|------|
| Licenc | $0 | Open source |
| API d√≠jak | $0 | Nincsenek |
| K√©r√©si limit | $0 | Soft limit (~60/perc) |
| Fejleszt√©si id≈ë | Magas | Karbantart√°s sz√ºks√©ges |

**P√©lda kalkul√°ci√≥:** Napi 1000 r√©szv√©ny √ó 365 nap = 365,000 k√©r√©s/√©v
- Hivatalos API: ~$275/√©v
- Cloudscraper: $0/√©v

## üöÄ Funkcionalit√°s √∂sszehasonl√≠t√°sa

### 1. El√©rhet≈ë adatok

| Adatt√≠pus | Cloudscraper | Hivatalos API | El≈ëny |
|-----------|--------------|--------------|-----------|
| Historikus OHLCV | ‚úÖ Max 60 √©v | ‚úÖ Max 30 nap (ingyenes) | Scraper |
| Intraday (1m, 5m) | ‚úÖ 30 nap | ‚úÖ 7 nap | Scraper |
| Real-time √°rfolyam | ‚úÖ Igen | ‚úÖ Igen | Egyenl≈ë |
| Options l√°ncok | ‚úÖ Teljes | ‚ö†Ô∏è Korl√°tozott | Scraper |
| Fundament√°lis adatok | ‚úÖ Teljes | ‚ö†Ô∏è Csomag f√ºgg≈ë | Scraper |
| Elemz≈ëi √©rt√©kel√©sek | ‚úÖ Igen | ‚ùå Csak Premium | Scraper |
| Insider keresked√©s | ‚úÖ Igen | ‚ùå Csak Premium | Scraper |

### 2. Technikai param√©terek

| Param√©ter | Cloudscraper | Hivatalos API |
|-----------|--------------|--------------|
| Max id≈ëtartam | "max" (IPO-t√≥l) | 2 √©v (standard) |
| Intervallumok | 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo | Csomag f√ºgg≈ë |
| Batch k√©r√©sek | Nem | Igen (Premium) |
| WebSocket | Nem | Igen (Premium) |

## üìà Konkr√©t p√©ld√°k - Mit NEM tudsz az ingyenes hivatalos API-val

### 1. Hossz√∫ id≈ët√°v√∫ historikus adatok

```python
# Cloudscraper - M≈∞K√ñDIK
scraper.get("chart/AAPL?range=max")  # Minden adat 1980-t√≥l

# Hivatalos API Basic - NEM M≈∞K√ñDIK
api.get_history(period="max")  # Hiba: Max 1 h√≥nap az ingyenes verzi√≥ban
```

### 2. R√©szletes fundament√°lis adatok

```python
# Cloudscraper - TELJES p√©nz√ºgyi adatok
data = scraper.get("/quote/AAPL/financials")
# Eredm√©ny: √ñsszes negyed√©ves jelent√©s 5 √©vre visszamen≈ëleg

# Hivatalos API Basic - KORL√ÅTOZOTT
data = api.get_financials("AAPL")  
# Eredm√©ny: Csak kulcs mutat√≥k, TTM (trailing twelve months)
```

### 3. Opci√≥s adatok

```python
# Cloudscraper - TELJES opci√≥s l√°nc
options = scraper.get("/quote/AAPL/options")
# Minden lej√°rat, minden strike √°r

# Hivatalos API - FIZET≈êS
# $50+/h√≥ az opci√≥s adatok√©rt
```

## üéØ Val√≥s felhaszn√°l√°si esetek

### Mikor haszn√°lj Cloudscraper-t?

#### 1. Backtesting/Kutat√°s

```python
# 10 √©ves historikus adat 500 r√©szv√©nyre
# Hivatalos API k√∂lts√©g: ~$150/h√≥
# Cloudscraper: $0

for ticker in sp500_list:
    data = scraper.get(f"chart/{ticker}?range=10y&interval=1d")
    # 10 √©v √ó 252 keresked√©si nap = 2520 adatpont/r√©szv√©ny
```

#### 2. Akad√©miai/Hobbi projektek

- Egyetemi kutat√°s
- Szem√©lyes portf√≥li√≥ k√∂vet≈ë
- Keresked√©si bot fejleszt√©s/tesztel√©s
- G√©pi tanul√°s tan√≠t√≥ adatok

#### 3. Sz≈±r≈ë alkalmaz√°sok

```python
# Fundament√°lis sz≈±r≈ë - teljes S&P 500
for ticker in sp500:
    financials = scraper.get(f"/quote/{ticker}/key-statistics")
    # P/E, P/B, ROE, Debt/Equity, stb.
# Hivatalos API: Premium csomag sz√ºks√©ges
```

### Mikor NE haszn√°lj Cloudscraper-t?

#### 1. √âles keresked√©si rendszer

```python
# ‚ùå ROSSZ: Kritikus keresked√©si d√∂nt√©sek
if scraper_data['price'] < limit:  
    execute_trade(1000000)  # Mi van, ha timeout/hiba l√©p fel?
```

#### 2. Kereskedelmi szolg√°ltat√°s

```python
# ‚ùå ROSSZ: SaaS alkalmaz√°s
class TradingPlatform:
    def get_user_data(self):
        return scraper.get(...)  # ToS megs√©rt√©s, jogi kock√°zat
```

#### 3. Nagy frekvenci√°j√∫/alacsony k√©sleltet√©s≈± keresked√©s

```python
# ‚ùå ROSSZ: HFT vagy arbitrage
while True:
    price = scraper.get(...)  # 200-500ms k√©sleltet√©s
    # Hivatalos WebSocket: <10ms
```

## üìä Teljes√≠tm√©ny √∂sszehasonl√≠t√°sa

| Metrika | Cloudscraper | Hivatalos API |
|--------|--------------|--------------|
| K√©sleltet√©s | 200-500ms | 50-100ms |
| Megb√≠zhat√≥s√°g | ~95% | ~99.9% |
| Rate limit | ~60/perc | Csomag f√ºgg≈ë |
| P√°rhuzamos k√©r√©sek | 5-10 | 100+ (Premium) |
| Uptime garancia | Nincs | SLA 99.9% |

## üîß Gyakorlati megold√°s: hibrid megk√∂zel√≠t√©s

```python
class HybridDataProvider:
    """Egyes√≠ti mindk√©t megold√°s el≈ënyeit"""
    
    def __init__(self):
        self.scraper = cloudscraper.create_scraper()
        self.official_api = YahooFinanceAPI(key="...")
        self.cache = {}
    
    def get_realtime_quote(self, ticker):
        """Kritikus real-time adat - hivatalos API"""
        return self.official_api.get_quote(ticker)
    
    def get_historical_data(self, ticker, years=10):
        """Nem kritikus historikus adat - scraper"""
        if ticker in self.cache:
            return self.cache[ticker]
        
        try:
            # El≈ësz√∂r pr√≥b√°ld a scraper-t (ingyenes)
            data = self.scraper.get(f"chart/{ticker}?range={years}y")
            self.cache[ticker] = data
            return data
        except:
            # Fallback a hivatalos API-ra
            return self.official_api.get_history(ticker, period="max")
    
    def get_critical_trade_signal(self, ticker):
        """Kritikus keresked√©si jel - CSAK hivatalos"""
        return self.official_api.get_realtime(ticker)
```

## üí° K√∂lts√©g-haszon elemz√©s

### Kis projekt (Hobbi/Kutat√°s)

- √âves k√∂lts√©g hivatalos API: $120-360
- Cloudscraper k√∂lts√©g: $0
- Megtakar√≠t√°s: $120-360/√©v
- Kock√°zat: Alacsony (nem kritikus)
- **D√∂nt√©s: ‚úÖ Cloudscraper**

### K√∂zepes projekt (Startup/Kisv√°llalat)

- √âves k√∂lts√©g hivatalos API: $900-2400
- Cloudscraper karbantart√°s: ~20 √≥ra/√©v √ó $50 = $1000
- Megtakar√≠t√°s: -$100 √©s $1400 k√∂z√∂tt
- Kock√°zat: K√∂zepes
- **D√∂nt√©s: ‚ö†Ô∏è Hibrid megold√°s**

### Nagy projekt (Nagyv√°llalat)

- √âves k√∂lts√©g hivatalos API: $10,000+
- Cloudscraper kock√°zat: Reput√°ci√≥, jogi, uptime
- Alternat√≠va: Bloomberg Terminal ($24,000/√©v)
- **D√∂nt√©s: ‚ùå Hivatalos API/Bloomberg**

## üéØ V√©gs≈ë k√∂vetkeztet√©s

### Cloudscraper el≈ënyei:

1. **INGYENES** - Ez a f≈ë ok
2. **T√∂bb adat** - Hosszabb t√∂rt√©nelmi adatok, t√∂bb endpoint
3. **Nincs limit** - Csak soft rate limit
4. **Teljes hozz√°f√©r√©s** - Minden, ami a weben el√©rhet≈ë

### Hivatalos API el≈ënyei:

1. **Megb√≠zhat√≥s√°g** - SLA, t√°mogat√°s
2. **Sebess√©g** - Alacsonyabb k√©sleltet√©s
3. **Legalit√°s** - Nincs ToS (felhaszn√°l√°si felt√©telek) probl√©ma
4. **Stabilit√°s** - Nem t√∂rik el friss√≠t√©sek miatt
5. **T√°mogat√°s** - Van kihez fordulni probl√©m√°k eset√©n

### A val√≥s√°g:

- **80% hobbi projektek:** Cloudscraper
- **15% semi-professional:** Hibrid megold√°s
- **5% professional:** Hivatalos API

---

## üöÄ Kezd≈ë l√©p√©sek

### Cloudscraper telep√≠t√©se √©s haszn√°lata

#### Alaptelep√≠t√©s

```bash
# Telep√≠t√©s pip-pel
pip install cloudscraper

# Vagy requirements.txt-ben
echo "cloudscraper>=1.2.71" >> requirements.txt
pip install -r requirements.txt
```

#### Els≈ë l√©p√©sek k√≥ddal

```python
import cloudscraper
import json

# Scraper l√©trehoz√°sa
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'desktop': True
    }
)

# Yahoo Finance base URL
base_url = "https://query1.finance.yahoo.com/v8/finance"

# P√©lda: Historikus adatok lek√©r√©se
def get_historical_data(ticker, period="1y", interval="1d"):
    """
    Historikus OHLCV adatok lek√©r√©se
    
    Args:
        ticker: R√©szv√©ny szimb√≥lum (pl. "AAPL")
        period: Id≈ëszak (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
        interval: Intervallum (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo)
    """
    url = f"{base_url}/chart/{ticker}"
    params = {
        "range": period,
        "interval": interval,
        "includePrePost": "false"
    }
    
    response = scraper.get(url, params=params)
    data = response.json()
    
    return data['chart']['result'][0]

# P√©lda haszn√°lat
apple_data = get_historical_data("AAPL", period="1y", interval="1d")
print(json.dumps(apple_data, indent=2))
```

#### Halad√≥ haszn√°lat - t√∂bb endpoint

```python
# Real-time √°rfolyam
def get_quote(ticker):
    """Aktu√°lis √°rfolyam √©s alapadatok"""
    url = f"{base_url}/quote"
    params = {"symbols": ticker}
    response = scraper.get(url, params=params)
    return response.json()

# Fundament√°lis adatok
def get_financials(ticker):
    """P√©nz√ºgyi kimutat√°sok"""
    url = f"https://finance.yahoo.com/quote/{ticker}/financials"
    response = scraper.get(url)
    # HTML parsing sz√ºks√©ges (BeautifulSoup vagy lxml)
    return response.text

# Options l√°nc
def get_options_chain(ticker):
    """Opci√≥s l√°nc adatok"""
    url = f"{base_url}/options/{ticker}"
    response = scraper.get(url)
    return response.json()

# M√∫ltbeli dividend √©s split adatok
def get_dividends(ticker):
    """Osztal√©k t√∂rt√©net"""
    url = f"{base_url}/chart/{ticker}"
    params = {
        "range": "max",
        "interval": "1d",
        "events": "div,split"
    }
    response = scraper.get(url, params=params)
    return response.json()
```

#### Rate limiting √©s hibakezel√©s

```python
import time
from requests.exceptions import RequestException

def safe_get(scraper, url, max_retries=3, delay=1):
    """
    Biztons√°gos GET k√©r√©s retry logik√°val
    """
    for attempt in range(max_retries):
        try:
            response = scraper.get(url)
            response.raise_for_status()
            
            # Rate limiting - ne bomb√°zzuk a szervert
            time.sleep(delay)
            
            return response.json()
        except RequestException as e:
            if attempt == max_retries - 1:
                raise
            print(f"Hiba t√∂rt√©nt, √∫jrapr√≥b√°l√°s {attempt + 1}/{max_retries}...")
            time.sleep(delay * (attempt + 1))
```

### Hivatalos API Be√°ll√≠t√°sa (RapidAPI)

#### Regisztr√°ci√≥ √©s API kulcs beszerz√©se

1. Regisztr√°ci√≥ a [RapidAPI](https://rapidapi.com/) oldalon
2. Keres√©s: "Yahoo Finance API"
3. V√°lassz egy csomagot (Basic $0-t√≥l)
4. M√°sold ki az API kulcsot a Dashboard-r√≥l

#### Python haszn√°lat hivatalos API-val

```bash
# Telep√≠tsd a requests k√∂nyvt√°rat
pip install requests python-dotenv
```

```python
import requests
import os
from dotenv import load_dotenv

# T√∂ltsd be a k√∂rnyezeti v√°ltoz√≥kat
load_dotenv()

class YahooFinanceAPI:
    def __init__(self):
        self.api_key = os.getenv('RAPIDAPI_KEY')
        self.base_url = "https://yahoo-finance15.p.rapidapi.com"
        self.headers = {
            "X-RapidAPI-Key": self.api_key,
            "X-RapidAPI-Host": "yahoo-finance15.p.rapidapi.com"
        }
    
    def get_quote(self, ticker):
        """Real-time √°rfolyam"""
        url = f"{self.base_url}/api/v1/markets/quote"
        params = {"ticker": ticker}
        response = requests.get(url, headers=self.headers, params=params)
        return response.json()
    
    def get_historical(self, ticker, period="1mo"):
        """Historikus adatok (limit√°lt az ingyenes verzi√≥ban)"""
        url = f"{self.base_url}/api/v1/markets/stock/history"
        params = {
            "symbol": ticker,
            "interval": "1d",
            "diffandsplits": "false"
        }
        response = requests.get(url, headers=self.headers, params=params)
        return response.json()

# Haszn√°lat
api = YahooFinanceAPI()
quote = api.get_quote("AAPL")
print(quote)
```

#### .env f√°jl be√°ll√≠t√°sa

```bash
# Hozz l√©tre egy .env f√°jlt a projekt gy√∂k√©rk√∂nyvt√°r√°ban
RAPIDAPI_KEY=your_api_key_here
```

### F√ºgg≈ës√©gek

#### Cloudscraper megold√°shoz

```txt
cloudscraper>=1.2.71
requests>=2.28.0
beautifulsoup4>=4.11.0  # HTML parsing-hez
lxml>=4.9.0  # Gyorsabb HTML parsing
pandas>=1.5.0  # Adatelemz√©shez (opcion√°lis)
```

#### Hivatalos API megold√°shoz

```txt
requests>=2.28.0
python-dotenv>=0.21.0  # K√∂rnyezeti v√°ltoz√≥k kezel√©s√©hez
```

### Docker haszn√°lat (opcion√°lis)

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "your_script.py"]
```

```bash
# Build
docker build -t yahoo-finance-scraper .

# Run
docker run -e RAPIDAPI_KEY=your_key yahoo-finance-scraper
```

## üìö Tov√°bbi hasznos inform√°ci√≥k

### El√©rhet≈ë Yahoo Finance endpointok

#### Cloudscraper-rel el√©rhet≈ë f≈ëbb endpointok:

```python
# 1. Chart/Historikus adatok
f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}"
# Param√©terek: range, interval, events (div, split)

# 2. Quote - Real-time √°r
f"https://query1.finance.yahoo.com/v7/finance/quote?symbols={ticker}"

# 3. Options
f"https://query1.finance.yahoo.com/v7/finance/options/{ticker}"

# 4. Spark (mini chart adatok)
f"https://query1.finance.yahoo.com/v7/finance/spark?symbols={ticker}"

# 5. News
f"https://query1.finance.yahoo.com/v1/finance/search?q={ticker}"

# 6. Trending tickers
"https://query1.finance.yahoo.com/v1/finance/trending/US"

# 7. Market summary
"https://query1.finance.yahoo.com/v6/finance/quote/marketSummary"

# 8. Screener eredm√©nyek
"https://query1.finance.yahoo.com/v1/finance/screener/predefined/saved"
```

### Gyakori hib√°k √©s megold√°saik

#### 1. Rate limiting (429 hiba)

```python
import time
from functools import wraps

def rate_limit(max_per_minute=60):
    """Dekor√°tor rate limiting-hez"""
    min_interval = 60.0 / max_per_minute
    
    def decorator(func):
        last_called = [0.0]
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        
        return wrapper
    return decorator

# Haszn√°lat
@rate_limit(max_per_minute=50)
def get_data(ticker):
    return scraper.get(f"chart/{ticker}")
```

#### 2. Cloudflare v√©delem megker√ºl√©se

```python
# Halad√≥ scraper konfigur√°ci√≥
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'desktop': True
    },
    delay=10,  # K√©sleltet√©s Cloudflare kih√≠v√°s megold√°s√°hoz
    captcha={
        'provider': '2captcha',  # Opcion√°lis: captcha szolg√°ltat√°s
        'api_key': 'your_api_key'
    }
)
```

#### 3. Timeout hib√°k kezel√©se

```python
from requests.exceptions import Timeout

def get_with_timeout(url, timeout=10, retries=3):
    """Timeout kezel√©ssel rendelkez≈ë GET k√©r√©s"""
    for i in range(retries):
        try:
            response = scraper.get(url, timeout=timeout)
            return response.json()
        except Timeout:
            if i == retries - 1:
                raise
            print(f"Timeout, √∫jrapr√≥b√°l√°s... ({i+1}/{retries})")
            time.sleep(2 ** i)  # Exponenci√°lis backoff
```

### Adatok t√°rol√°sa √©s cache-el√©se

#### SQLite cache p√©lda

```python
import sqlite3
import json
from datetime import datetime, timedelta

class DataCache:
    def __init__(self, db_path='yahoo_cache.db'):
        self.conn = sqlite3.connect(db_path)
        self.create_tables()
    
    def create_tables(self):
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS cache (
                ticker TEXT,
                data_type TEXT,
                data TEXT,
                timestamp DATETIME,
                PRIMARY KEY (ticker, data_type)
            )
        ''')
    
    def get(self, ticker, data_type, max_age_hours=1):
        """Cache-b≈ël olvas, ha friss az adat"""
        cursor = self.conn.execute(
            'SELECT data, timestamp FROM cache WHERE ticker=? AND data_type=?',
            (ticker, data_type)
        )
        row = cursor.fetchone()
        
        if row:
            data, timestamp = row
            cache_time = datetime.fromisoformat(timestamp)
            if datetime.now() - cache_time < timedelta(hours=max_age_hours):
                return json.loads(data)
        
        return None
    
    def set(self, ticker, data_type, data):
        """Adatok cache-el√©se"""
        self.conn.execute(
            'INSERT OR REPLACE INTO cache VALUES (?, ?, ?, ?)',
            (ticker, data_type, json.dumps(data), datetime.now().isoformat())
        )
        self.conn.commit()

# Haszn√°lat
cache = DataCache()

def get_cached_data(ticker):
    # El≈ësz√∂r cache-b≈ël pr√≥b√°ld
    cached = cache.get(ticker, 'historical')
    if cached:
        return cached
    
    # Ha nincs cache-ben, lek√©rj√ºk
    data = scraper.get(f"chart/{ticker}?range=1y").json()
    cache.set(ticker, 'historical', data)
    return data
```

#### Redis cache (production k√∂rnyezethez)

```python
import redis
import json

class RedisCache:
    def __init__(self, host='localhost', port=6379, ttl=3600):
        self.redis = redis.Redis(host=host, port=port, decode_responses=True)
        self.ttl = ttl
    
    def get(self, key):
        data = self.redis.get(key)
        return json.loads(data) if data else None
    
    def set(self, key, value):
        self.redis.setex(key, self.ttl, json.dumps(value))

# Haszn√°lat
cache = RedisCache(ttl=3600)  # 1 √≥ra TTL

def get_quote_cached(ticker):
    cache_key = f"quote:{ticker}"
    cached = cache.get(cache_key)
    
    if cached:
        return cached
    
    data = scraper.get(f"quote?symbols={ticker}").json()
    cache.set(cache_key, data)
    return data
```

### Adatok feldolgoz√°sa Pandas-szal

```python
import pandas as pd
from datetime import datetime

def parse_yahoo_data(ticker, period="1y"):
    """Yahoo Finance adatok Pandas DataFrame-be"""
    response = scraper.get(
        f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}",
        params={"range": period, "interval": "1d"}
    )
    data = response.json()
    
    result = data['chart']['result'][0]
    timestamps = result['timestamp']
    quote = result['indicators']['quote'][0]
    
    df = pd.DataFrame({
        'date': pd.to_datetime(timestamps, unit='s'),
        'open': quote['open'],
        'high': quote['high'],
        'low': quote['low'],
        'close': quote['close'],
        'volume': quote['volume']
    })
    
    df.set_index('date', inplace=True)
    return df

# Haszn√°lat
df = parse_yahoo_data('AAPL', period='1mo')
print(df.head())

# Technikai indik√°torok sz√°m√≠t√°sa
df['SMA_20'] = df['close'].rolling(window=20).mean()
df['SMA_50'] = df['close'].rolling(window=50).mean()
df['daily_return'] = df['close'].pct_change()

# Export
df.to_csv('aapl_data.csv')
df.to_excel('aapl_data.xlsx')
```

### Teljes√≠tm√©ny optimaliz√°l√°s

#### P√°rhuzamos lek√©r√©sek

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def fetch_multiple_tickers(tickers, max_workers=5):
    """
    T√∂bb ticker p√°rhuzamos lek√©r√©se
    FIGYELEM: Ne √°ll√≠tsd t√∫l magasra a max_workers-t (rate limit!)
    """
    results = {}
    
    def fetch_ticker(ticker):
        try:
            data = scraper.get(f"chart/{ticker}?range=1d").json()
            return ticker, data
        except Exception as e:
            return ticker, None
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(fetch_ticker, t): t for t in tickers}
        
        for future in as_completed(futures):
            ticker, data = future.result()
            results[ticker] = data
            
    return results

# Haszn√°lat
sp500_sample = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']
data = fetch_multiple_tickers(sp500_sample, max_workers=3)
```

### Jogszab√°lyi √©s etikai megfontol√°sok

#### ‚öñÔ∏è Jogi szempontok

1. **Yahoo Finance ToS**: A scraping technikailag s√©rti a felhaszn√°l√°si felt√©teleket
2. **CFAA (Computer Fraud and Abuse Act)**: USA-ban potenci√°lisan illeg√°lis
3. **EU GDPR**: Szem√©lyes adatok kezel√©s√©re vonatkoz√≥ szab√°lyok
4. **Kereskedelmi haszn√°lat**: Kifejezetten tilos hivatalos enged√©ly n√©lk√ºl

#### ü§ù Etikus haszn√°lat

```python
# J√ì gyakorlatok
‚úÖ √âsszer≈± rate limiting (max 60 req/perc)
‚úÖ User-Agent be√°ll√≠t√°sa
‚úÖ Csak publikus adatok
‚úÖ Nem kereskedelmi c√©lra
‚úÖ Cache haszn√°lata az ism√©telt k√©r√©sek elker√ºl√©s√©re

# ROSSZ gyakorlatok  
‚ùå Agressz√≠v scraping (100+ req/perc)
‚ùå Fake User-Agent
‚ùå Kereskedelmi szolg√°ltat√°s alapja
‚ùå Adatok √∫jra √©rt√©kes√≠t√©se
‚ùå V√©dett tartalom let√∂lt√©se
```

### Alternat√≠v adatforr√°sok

Ha a Yahoo Finance nem elegend≈ë vagy probl√©m√°s:

```python
# Alpha Vantage API (ingyenes tier)
# https://www.alphavantage.co
# Limit: 5 k√©r√©s/perc, 500 k√©r√©s/nap

# Finnhub API (ingyenes tier)
# https://finnhub.io
# Limit: 60 k√©r√©s/perc

# IEX Cloud (ingyenes tier)
# https://iexcloud.io
# Limit: 50,000 k√©r√©s/h√≥

# Polygon.io (fizet≈ës)
# https://polygon.io
# Limit: Nincs (tier f√ºgg≈ë)

# yfinance Python k√∂nyvt√°r
pip install yfinance
import yfinance as yf
ticker = yf.Ticker("AAPL")
hist = ticker.history(period="1mo")
```

### Monitoring √©s napl√≥z√°s

```python
import logging
from datetime import datetime

# Logging be√°ll√≠t√°sa
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('yahoo_scraper.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger('YahooScraper')

def monitored_request(ticker):
    """Monitorozott k√©r√©s napl√≥z√°ssal"""
    start_time = datetime.now()
    
    try:
        logger.info(f"Fetching data for {ticker}")
        response = scraper.get(f"chart/{ticker}")
        
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"Success: {ticker} in {elapsed:.2f}s")
        
        return response.json()
    except Exception as e:
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.error(f"Error fetching {ticker} after {elapsed:.2f}s: {str(e)}")
        raise
```

### Tesztel√©s

```python
import unittest
from unittest.mock import patch, Mock

class TestYahooScraper(unittest.TestCase):
    
    def setUp(self):
        self.scraper = cloudscraper.create_scraper()
    
    @patch('cloudscraper.CloudScraper.get')
    def test_get_quote(self, mock_get):
        """Quote endpoint tesztel√©se"""
        mock_response = Mock()
        mock_response.json.return_value = {
            'quoteResponse': {
                'result': [{'symbol': 'AAPL', 'regularMarketPrice': 150.0}]
            }
        }
        mock_get.return_value = mock_response
        
        result = get_quote('AAPL')
        self.assertEqual(result['quoteResponse']['result'][0]['symbol'], 'AAPL')
    
    def test_rate_limiting(self):
        """Rate limiting m≈±k√∂d√©s√©nek tesztel√©se"""
        start = time.time()
        
        @rate_limit(max_per_minute=30)
        def dummy_request():
            return True
        
        # 3 k√©r√©s
        for _ in range(3):
            dummy_request()
        
        elapsed = time.time() - start
        # Legal√°bb 4 m√°sodperc kellene (2 sec/k√©r√©s 30/perc limit mellett)
        self.assertGreater(elapsed, 4)

if __name__ == '__main__':
    unittest.main()
```

## Fontos
- A web scraping technikailag s√©rtheti a Yahoo Finance szolg√°ltat√°si felt√©teleit
- Kereskedelmi felhaszn√°l√°shoz mindig szerezz hivatalos enged√©lyt
- Ez a dokumentum nem jelent jogi tan√°csad√°st
- A szerz≈ëk nem v√°llalnak felel≈ëss√©get az itt le√≠rt m√≥dszerek haszn√°lat√°b√≥l ered≈ë k√∂vetkezm√©nyek√©rt

## üìû Kapcsolat √©s tov√°bbi forr√°sok

- [Yahoo Finance hivatalos oldal](https://finance.yahoo.com)
- [RapidAPI Yahoo Finance API](https://rapidapi.com/apidojo/api/yahoo-finance1)
- [Cloudscraper GitHub](https://github.com/venomous/cloudscraper)
- [yfinance k√∂nyvt√°r](https://github.com/ranaroussi/yfinance)
- [Pandas dokument√°ci√≥](https://pandas.pydata.org/docs/)
